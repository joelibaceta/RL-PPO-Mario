# PPO Mario Agent üïπÔ∏è

Este proyecto implementa un agente **Proximal Policy Optimization (PPO)** para jugar *Super Mario Bros*, utilizando PyTorch y un entorno personalizado basado en `gym-super-mario-bros`. Incluye una CNN como policy network, normalizaci√≥n de recompensas y varios mecanismos para estabilidad del entrenamiento.

---

## üìú Contenido

- [Arquitectura CNN](#arquitectura-cnn)
- [Entrenador PPO](#entrenador-ppo)
- [Hiperpar√°metros](#hiperpar√°metros)
- [Construcci√≥n del entorno](#construcci√≥n-del-entorno)
- [Uso](#uso)

---

## üß† Arquitectura CNN

La policy/value network usa una CNN ligera que procesa stacks de 4 frames para capturar informaci√≥n temporal.

```
Input: (4, 84, 84)

Conv2D(4, 32, kernel_size=8, stride=4) ‚Üí ReLU
Conv2D(32, 64, kernel_size=4, stride=2) ‚Üí ReLU
Conv2D(64, 64, kernel_size=3, stride=1) ‚Üí ReLU
Flatten
FC(3136, 512) ‚Üí ReLU
Output policy logits: FC(512, n_actions)
Output value: FC(512, 1)
```

- **Policy head**: genera logits para la distribuci√≥n categ√≥rica sobre acciones.
- **Value head**: estima el valor del estado para ventaja.

---

## üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenador PPO

El entrenamiento sigue un ciclo:

1. **Rollout**:

   - Ejecuta la pol√≠tica actual durante `num_steps` en `num_envs` entornos paralelos.
   - Guarda: observaciones, acciones, logprobs, recompensas normalizadas.

2. **Ventaja (GAE)**:

   - Estima ventajas con *Generalized Advantage Estimation*.

3. **Actualizaci√≥n PPO**:

   - Calcula la p√©rdida de pol√≠tica con clipping: \(\min(ratio \cdot A, clip(ratio) \cdot A)\)
   - A√±ade p√©rdida de valor y entrop√≠a.
   - Early stopping o rollback si el KL-divergence supera el target.

4. **Logging**:

   - Guarda m√©tricas en TensorBoard: reward promedio, max\_x\_pos, entrop√≠a, proporci√≥n de acciones.

---

## ‚öôÔ∏è Hiperpar√°metros

| Par√°metro         | Valor     | Descripci√≥n                                                |
| ----------------- | --------- | ---------------------------------------------------------- |
| `total_timesteps` | 1,000,000 | Total de pasos de entrenamiento.                           |
| `lr`              | 2.5e-4    | Learning rate inicial, decae en el tiempo.                 |
| `gamma`           | 0.99      | Factor de descuento de recompensas.                        |
| `gae_lambda`      | 0.98      | Mezcla bias-variance en ventajas.                          |
| `clip_coef`       | 0.2       | L√≠mite de clipping en ratio PPO.                           |
| `entropy_coef`    | Din√°mico  | Incentiva la exploraci√≥n (disminuye con los updates).      |
| `epsilon`         | Din√°mico  | Probabilidad de exploraci√≥n aleatoria, nunca menor a 0.05. |
| `num_envs`        | 8         | N√∫mero de entornos paralelos para colectar experiencias.   |
| `num_steps`       | 128       | Pasos por rollout antes de actualizar.                     |

**T√©cnicas adicionales**:

- Normalizaci√≥n online de recompensas.
- Penalizaci√≥n a retrocesos (para evitar que el agente se quede bloqueado).
- Bonificaciones por milestones en X para incentivar el progreso.
- Decaimiento de learning rate.

---

## üå± Construcci√≥n del entorno

El entorno utiliza *wrappers* personalizados y de `gym` para preparar los datos de entrada y gestionar la interacci√≥n del agente con el juego:

- **FrameStack**: apila 4 frames para incluir informaci√≥n temporal.
- **GrayScaleObservation**: convierte a escala de grises para simplificar la entrada.
- **ResizeObservation**: reduce las im√°genes a 84x84 p√≠xeles para eficiencia.
- **FrameSkipWrapper**: omite frames para acelerar la simulaci√≥n.
- **FrameCropWrapper**: recorta la HUD para evitar informaci√≥n redundante.
- **LifeResetWrapper** (opcional): reinicia el entorno al perder una vida.
- **FilterColorsWrapper** (opcional): filtra colores espec√≠ficos.
- **Vector Envs (Sync/Async)**: ejecuta m√∫ltiples copias del entorno en paralelo para mejorar la eficiencia de recolecci√≥n de datos.

Estos *wrappers* permiten un procesamiento m√°s r√°pido y estable durante el entrenamiento.

---

## üöÄ Uso

1. Instalar dependencias:

```bash
pip install -r requirements.txt
```

2. Entrenar el agente:

```bash
python cli.py
```

3. Visualizar m√©tricas en TensorBoard:

```bash
tensorboard --logdir=runs
```

4. Evaluar el modelo entrenado:

```bash
python evaluate.py --model ppo_mario.pth
```

---

## üìà M√©tricas registradas

- `episode_reward`: Recompensa promedio por episodio.
- `max_x_pos`: M√°ximo progreso horizontal alcanzado.
- `avg_x_pos`: Progreso promedio por update.
- `policy_entropy`: Diversidad de acciones elegidas.
- Proporci√≥n de cada acci√≥n.

---

## üìå Notas

- El agente usa PPO con **early stopping** y rollback para evitar colapsos.


## üí° Sugerencias de mejoras

Progreso hasta el momento:
<center>
     <img src="figs/Progreso.gif" alt="Progreso del agente" width="300">
</center>

- Implementar una etapa de pre-entrenamiento con movimientos aleatorios para diversificar la experiencia inicial.
- Implementar una etapa sin movimiento para dejar que el entorno cambie y diversificar la experiencia.
- Probar con diferentes arquitecturas de CNN y comparar resultados (Quizas incluir una capa LSTM).
- Probar diferentes combinaciones de hiperpar√°metros y documentar resultados.
- Crear un script para entrenamiento m√∫ltiple con hiperpar√°metros variados.
- Grabar un video del mejor episodio durante el entrenamiento (con OpenCV).
- Agregar m√©tricas adicionales: epsilon, learning rate, duraci√≥n de episodios.
- Probar con el rango de acciones `RIGHT_ONLY` en lugar de `SIMPLE_MOVEMENT`.
- Incorporar recompensas progresivas por superar posiciones X (checkpoints: 1500, 2000, 3000).
- Penalizar retrocesos o quedarse quieto demasiado tiempo.
- Incluir recompensas por recolectar monedas o eliminar enemigos.
- Experimentar con un *replay buffer* para estabilizar el aprendizaje.