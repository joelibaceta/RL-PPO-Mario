# PPO Mario Agent üïπÔ∏è

Este proyecto implementa un agente **Proximal Policy Optimization (PPO)** para jugar *Super Mario Bros*, utilizando PyTorch y un entorno personalizado basado en `gym-super-mario-bros`. Incluye una CNN como policy network, normalizaci√≥n de recompensas y varios mecanismos para estabilidad del entrenamiento.

---

## üìú Contenido

- [Arquitectura CNN](#arquitectura-cnn)
- [Entrenador PPO](#entrenador-ppo)
- [Hiperpar√°metros](#hiperpar√°metros)
- [Construcci√≥n del entorno](#construcci√≥n-del-entorno)
- [Uso](#uso)

---

## üß† Arquitectura CNN

La policy/value network usa una CNN ligera que procesa stacks de 4 frames para capturar informaci√≥n temporal.

```
Input: (4, 84, 84)

Conv2D(4, 32, kernel_size=8, stride=4) ‚Üí ReLU
Conv2D(32, 64, kernel_size=4, stride=2) ‚Üí ReLU
Conv2D(64, 64, kernel_size=3, stride=1) ‚Üí ReLU
Flatten
FC(3136, 512) ‚Üí ReLU
Output policy logits: FC(512, n_actions)
Output value: FC(512, 1)
```

- **Policy head**: genera logits para la distribuci√≥n categ√≥rica sobre acciones.
- **Value head**: estima el valor del estado para ventaja.

---

## üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenador PPO

El entrenamiento sigue un ciclo:

1. **Rollout**:

   - Ejecuta la pol√≠tica actual durante `num_steps` en `num_envs` entornos paralelos.
   - Guarda: observaciones, acciones, logprobs, recompensas normalizadas.

2. **Ventaja (GAE)**:

   - Estima ventajas con *Generalized Advantage Estimation*.

3. **Actualizaci√≥n PPO**:

   - Calcula la p√©rdida de pol√≠tica con clipping: \(\min(ratio \cdot A, clip(ratio) \cdot A)\)
   - A√±ade p√©rdida de valor y entrop√≠a.
   - Early stopping o rollback si el KL-divergence supera el target.

4. **Logging**:

   - Guarda m√©tricas en TensorBoard: reward promedio, max\_x\_pos, entrop√≠a, proporci√≥n de acciones.

---

## ‚öôÔ∏è Hiperpar√°metros

| Par√°metro         | Valor     | Descripci√≥n                                                |
| ----------------- | --------- | ---------------------------------------------------------- |
| `total_timesteps` | 1,000,000 | Total de pasos de entrenamiento.                           |
| `lr`              | 2.5e-4    | Learning rate inicial, decae en el tiempo.                 |
| `gamma`           | 0.99      | Factor de descuento de recompensas.                        |
| `gae_lambda`      | 0.98      | Mezcla bias-variance en ventajas.                          |
| `clip_coef`       | 0.2       | L√≠mite de clipping en ratio PPO.                           |
| `entropy_coef`    | Din√°mico  | Incentiva la exploraci√≥n (disminuye con los updates).      |
| `epsilon`         | Din√°mico  | Probabilidad de exploraci√≥n aleatoria, nunca menor a 0.05. |
| `num_envs`        | 8         | N√∫mero de entornos paralelos para colectar experiencias.   |
| `num_steps`       | 128       | Pasos por rollout antes de actualizar.                     |

**T√©cnicas adicionales**:

- Normalizaci√≥n online de recompensas.
- Penalizaci√≥n a retrocesos (para evitar que el agente se quede bloqueado).
- Bonificaciones por milestones en X para incentivar el progreso.
- Decaimiento de learning rate.

---

## üå± Construcci√≥n del entorno

El entorno se compone de varias capas:

- ``: apila 4 frames para incluir informaci√≥n temporal.
- ``: convierte a escala de grises para simplificar la entrada.
- ``: reduce a 84x84 para eficiencia.
- ``: a√±ade bonificaciones y penalizaciones personalizadas.
- ``: ejecuta m√∫ltiples copias del entorno en paralelo.

---

## üöÄ Uso

1. Instalar dependencias:

```bash
pip install -r requirements.txt
```

2. Entrenar el agente:

```bash
python cli.py
```

3. Visualizar m√©tricas en TensorBoard:

```bash
tensorboard --logdir=runs
```

4. Evaluar el modelo entrenado:

```bash
python evaluate.py --model ppo_mario.pth
```

---

## üìà M√©tricas registradas

- `episode_reward`: Recompensa promedio por episodio.
- `max_x_pos`: M√°ximo progreso horizontal alcanzado.
- `avg_x_pos`: Progreso promedio por update.
- `policy_entropy`: Diversidad de acciones elegidas.
- Proporci√≥n de cada acci√≥n.

---

## üìå Notas

- El agente usa PPO con **early stopping** y rollback para evitar colapsos.


