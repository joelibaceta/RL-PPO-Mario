# Super Mario Bros con PPO: Modelado y Resultados

Este proyecto implementa un agente de Aprendizaje por Refuerzo Profundo (DRL) usando Proximal Policy Optimization (PPO) para aprender a jugar *Super Mario Bros*. Aprovecha un entorno vectorizado para acelerar el entrenamiento y t√©cnicas avanzadas para estabilizar el aprendizaje.

---

## 1. Modelado del Problema

El entrenamiento del agente para jugar *Super Mario Bros* se formula como un **Proceso de Decisi√≥n de Markov (MDP)**, definido como un cu√°druple \((S, A, P, R)\):

- **Espacio de Estados (****\(S\)****)**: Cada estado es un stack de \(n\) im√°genes consecutivas del entorno, preprocesadas a tama√±o reducido (84x84 p√≠xeles) y normalizadas para capturar la din√°mica temporal:

  $$
  s_t = \{I_{t-3}, I_{t-2}, I_{t-1}, I_t\}
  $$

  Este enfoque permite al agente inferir velocidades y trayectorias en un entorno parcialmente observable.

- **Espacio de Acciones (****\(A\)****)**: Conjunto discreto de combinaciones de botones del mando: moverse a la derecha/izquierda, saltar, disparar. El espacio de acci√≥n se simplifica para facilitar la convergencia.

- **Din√°mica de Transici√≥n (****\(P\)****)**: Parcialmente determinista: las acciones del agente interact√∫an con la f√≠sica del juego y el comportamiento limitado de los enemigos.

- **Funci√≥n de Recompensa (****\(R\)****)**:

  - Recompensa positiva por avance horizontal: \(+0.01 * \Delta x\)
  - Bonificaci√≥n al alcanzar la bandera: \(+5.0\)
  - Penalizaci√≥n por retroceso o muerte: \(-1.0\)

- **Horizonte Temporal**: Episodios de longitud variable, concluyen cuando el agente muere o completa un nivel.

---

## 2. Soluci√≥n Propuesta

### 2.1 ¬øPor qu√© Deep Reinforcement Learning (DRL)?

- **Alta dimensionalidad**: las entradas son im√°genes que requieren CNNs para la extracci√≥n de caracter√≠sticas.
- **Recompensas escasas** (*sparse rewards*): dificultan el aprendizaje con m√©todos tradicionales.
- **Ambiente parcialmente observable**: obliga a considerar el historial de frames.

‚úÖ **DRL** permite:

- Procesar datos visuales complejos.
- Generalizar a nuevos escenarios.
- Aprender pol√≠ticas en espacios de acci√≥n no triviales.

üìñ *Sutton y Barto* (2018) recomiendan DRL en entornos con estas caracter√≠sticas [1].

---

### 2.2 ¬øPor qu√© Proximal Policy Optimization (PPO) frente a DQN?

**DQN (Deep Q-Network)**:

- ‚úÖ Eficiente en entornos peque√±os como *Pong*.
- ‚ùå Limitaciones en *Mario Bros*:
  - Sobreestimaci√≥n de valores Q.
  - Exploraci√≥n pobre en espacios grandes.
  - Requiere t√©cnicas adicionales (Double DQN, Prioritized Replay) que complican su uso.

üìÑ *Smith, 2025* muestra que PPO supera a DQN en *Super Mario Bros* por su manejo estable de pol√≠ticas estoc√°sticas [2].

---

### üß™ Ventajas de PPO

1. **Clipping de actualizaciones** para evitar pol√≠ticas divergentes.
2. **Pol√≠tica estoc√°stica** que favorece la exploraci√≥n.
3. **Simetr√≠a actor-cr√≠tico** para aprender simult√°neamente la pol√≠tica y el valor.
4. **F√°cil tuning** comparado con TRPO y otros m√©todos.

---

## 3. Arquitectura de la Aplicaci√≥n

### 3.1 Red Neuronal Convolucional (CNN)

El agente implementa una arquitectura **actor-cr√≠tico compartida** que combina un extractor convolucional y dos cabezas densas:

```
Input: (4, 84, 84)  # Stack de 4 frames

Conv2D(4, 32, kernel_size=8, stride=4)  ‚Üí ReLU
Conv2D(32, 64, kernel_size=4, stride=2) ‚Üí ReLU
Conv2D(64, 64, kernel_size=3, stride=1) ‚Üí ReLU
Flatten
FC(3136, 512)                           ‚Üí ReLU

Policy head: FC(512, n_actions)         # Logits para distribuci√≥n categ√≥rica
Value head:  FC(512, 1)                  # Escalar para estimar V(s)
```

**Detalles t√©cnicos:**

- **Stack de frames**: captura la din√°mica temporal.
- **Activaciones ReLU**: mitigan gradientes desvanecidos.
- **Cabezas separadas**: PPO requiere la pol√≠tica \(\pi_\theta(a|s)\) y una estimaci√≥n del valor \(V_\phi(s)\).

---

### 3.2 Ciclo de Entrenamiento

1. **Rollout**:
   - Ejecuta \(\pi_\theta\) durante `num_steps` en `num_envs` entornos vectorizados.
2. **Estimaci√≥n de ventajas (GAE)**:
   $$
   \hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \dots
   $$
3. **Actualizaci√≥n PPO**:
   $$
   L(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
   $$

---

### 3.3 Hiperpar√°metros

| Par√°metro         | Valor                 | Descripci√≥n                                               |
| ----------------- | --------------------- | --------------------------------------------------------- |
| `total_timesteps` | 1,000,000             | N√∫mero total de pasos de entrenamiento.                   |
| `lr`              | \$2.5 \cdot 10^{-4}\$ | *Learning rate* inicial, decae con los updates.           |
| \$\gamma\$        | 0.99                  | Factor de descuento para recompensas futuras.             |
| \$\lambda\$ (GAE) | 0.95                  | Par√°metro \$\lambda\$ en GAE para balance sesgo/varianza. |
| `clip_coef`       | 0.2                   | L√≠mite de *clipping* en el ratio PPO.                     |
| `num_envs`        | 8                     | N√∫mero de entornos paralelos para colectar experiencias.  |
| `num_steps`       | 128                   | Pasos por rollout antes de actualizar la pol√≠tica.        |

---

### 3.4 T√©cnicas Adicionales

#### 1. Normalizaci√≥n de Recompensas

- Ajusta la media y varianza acumuladas para estabilizar el entrenamiento.

#### 2. Annealing de Entrop√≠a

- Reduce gradualmente el t√©rmino de entrop√≠a para pasar de exploraci√≥n a explotaci√≥n.

#### 3. KL Penalty Adaptativo

- Ajusta din√°micamente \(\beta_{KL}\) para controlar la divergencia entre pol√≠ticas.

#### 4. Exploraci√≥n Epsilon-Greedy

- Introduce una \(\epsilon\) decreciente para evitar estancamientos.

#### 5. Entornos Vectorizados

- Uso de 8 entornos paralelos para acelerar la recolecci√≥n de datos.

---

## üìö Referencias

[1] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*, MIT Press, 2018. [2] J. Smith, "Comparative analysis of DQN and PPO on Super Mario Bros.", Stanford CS224R Report, 2025. [3] J. Schulman et al., "Proximal Policy Optimization Algorithms", arXiv:1707.06347, 2017.

